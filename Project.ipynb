import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Final_Project/Dataset.csv")
a=df
b=df
print(b.head())

# **Exploratory Data Analysis**

# Display basic information about the dataset
print(df.info())

# Display summary statistics for numerical columns
print(df.describe())

# Check for missing values
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder



# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply LabelEncoder to the categorical column
df['tfopwg_disp'] = label_encoder.fit_transform(df['tfopwg_disp'])
b['tfopwg_disp'] = label_encoder.fit_transform(b['tfopwg_disp'])

# Print the DataFrame with encoded categorical column
print(df)
print(b)

# Fill missing values in the 'date' column using forward fill
df['toi_updated'] = df['toi_updated'].fillna(method='ffill')

# Print the DataFrame to check the filled values
print(df)

a.drop('toi_updated', axis=1, inplace=True)
print(a)

a_filled = a.fillna(a.mean())
print(a_filled)
df_filled = df.fillna(df.mean())
print(df_filled)

# Visualize correlations between numerical features using a heatmap
plt.figure(figsize=(20,14))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Example: Scatter plot of 'pl_rade' vs. 'pl_orbper' with hue based on 'tfopwg_disp'
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='pl_rade', y='pl_orbper', hue='tfopwg_disp')
plt.title('Scatter Plot: Exoplanet Radius vs. Orbital Period')
plt.xlabel('Radius (Earth radii)')
plt.ylabel('Orbital Period (days)')
plt.legend(title='Exoplanet Status')
plt.show()

### Model Evaluation

# **Analysis**

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply LabelEncoder to the categorical column
a['tfopwg_disp'] = label_encoder.fit_transform(a['tfopwg_disp'])

# Print the DataFrame with encoded categorical column
b=print(a)
b

# Parse 'toi_updated' column into datetime format
df['toi_updated'] = pd.to_datetime(df['toi_updated'], format='%d-%m-%Y')

# Fill missing values in all numeric columns using forward fill
numeric_columns = df.select_dtypes(include=['number']).columns
df[numeric_columns] = df[numeric_columns].fillna(method='ffill')
df['toi_updated'] = df['toi_updated'].interpolate(method='linear')

# Save the DataFrame to a new CSV file
df.to_csv('/content/drive/MyDrive/Final_Project/new_dataset.csv', index=False)  # Update the path as needed
print(df)

import pandas as pd
from sklearn.ensemble import IsolationForest

def detect_anomalies(df):
    """
    Detect anomalies in the dataset using the Isolation Forest algorithm.

    Parameters:
    - df: pandas DataFrame, the cleaned dataset.

    Returns:
    - anomalies: pandas DataFrame, the detected anomalies.
    """
    # Initialize the Isolation Forest model
    model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)

    # Fit the model and predict anomalies
    df['anomaly'] = model.fit_predict(df.drop(columns=['toi_updated']))

    # Filter the anomalies
    anomalies = df[df['anomaly'] == -1]

    return anomalies

def main():
    """
    Main function to detect anomalies in the cleaned dataset.
    """
    # Assuming 'df' is your cleaned dataset
    # Update 'df' with your actual cleaned dataset variable
    df = pd.read_csv('/content/drive/MyDrive/Final_Project/new_dataset.csv')

    if df is not None:
        # Detect anomalies
        anomalies = detect_anomalies(df)

        # Display the anomalies
        print("Detected Anomalies:")
        print(anomalies.head())
    else:
        print("Failed to load the cleaned dataset.")

if __name__ == "__main__":
    main()

# Function to classify exoplanets based on their properties
def classify_exoplanets(exoplanet_data):
    """
    Classify exoplanets into different categories based on their properties.

    Parameters:
    exoplanet_data (dict): Dictionary containing exoplanet data with keys as property names.

    Returns:
    classification (str): Category to which the exoplanet belongs.
    """

    # Extract relevant properties from exoplanet_data
    orb_per = exoplanet_data.get('pl_orbper', 0)  # Orbital period
    rade = exoplanet_data.get('pl_rade', 0)  # Planet radius
    eqt = exoplanet_data.get('pl_eqt', 0)  # Equilibrium temperature

    # Classify exoplanets based on their properties
    if orb_per < 10 and rade < 1 and eqt > 1000:
        classification = 'Hot Super-Earth'
    elif orb_per > 100 and rade > 5 and eqt < 500:
        classification = 'Cold Giant Planet'
    else:
        classification = 'Unclassified'

    return classification

# Example exoplanet data
exoplanet_data = {
    'pl_orbper': 5,  # Orbital period in days
    'pl_rade': 0.8,  # Planet radius in Earth radii
    'pl_eqt': 1200  # Equilibrium temperature in Kelvin
}

# Classify the example exoplanet data
result = classify_exoplanets(exoplanet_data)
print(result)  # Output: Hot Super-Earth

# Function to classify exoplanets based on their properties
def classify_exoplanets(exoplanet_data):
    """
    Classify exoplanets into different categories based on their properties.

    Parameters:
    exoplanet_data (dict): Dictionary containing exoplanet data with keys as property names.

    Returns:
    classification (str): Category to which the exoplanet belongs.
    """

    # Extract relevant properties from exoplanet_data
    orb_per = exoplanet_data.get('pl_orbper', 0)  # Orbital period
    rade = exoplanet_data.get('pl_rade', 0)  # Planet radius
    eqt = exoplanet_data.get('pl_eqt', 0)  # Equilibrium temperature

    # Classify exoplanets based on their properties
    if orb_per < 10 and rade < 1 and eqt > 1000:
        classification = 'Hot Super-Earth'
    elif orb_per > 100 and rade > 5 and eqt < 500:
        classification = 'Cold Giant Planet'
    else:
        classification = 'Unclassified'

    return classification

# User input for exoplanet properties
orb_per = float(input("Enter the orbital period in days (between 0 and 1000): "))
rade = float(input("Enter the planet radius in Earth radii (between 0 and 10): "))
eqt = float(input("Enter the equilibrium temperature in Kelvin (between 0 and 2000): "))

# Check if input values are within the specified range
if 0 <= orb_per <= 1000 and 0 <= rade <= 10 and 0 <= eqt <= 2000:
    # Create exoplanet data dictionary
    user_exoplanet_data = {
        'pl_orbper': orb_per,
        'pl_rade': rade,
        'pl_eqt': eqt
    }

    # Classify the user input exoplanet data
    result = classify_exoplanets(user_exoplanet_data)
    print("Classification:", result)
else:
    print("Please enter valid values within the specified range.")


def classify_exoplanets(data):
    """
    Classify exoplanets into different categories based on their properties.

    Args:
    data (dict): Dictionary containing exoplanet data with keys as column names.

    Returns:
    str: Classification of the exoplanet based on its properties.
    """

    # Check if all required keys are present in the data dictionary
    required_keys = ['pl_orbper', 'pl_rade', 'st_teff']
    if not all(key in data for key in required_keys):
        raise ValueError("Missing required keys in data dictionary.")

    # Extracting relevant data from the dictionary
    pl_orbper = data['pl_orbper']
    pl_rade = data['pl_rade']
    st_teff = data['st_teff']

    # Classify exoplanets based on their properties
    if 200 <= pl_orbper <= 500 and 1 <= pl_rade <= 2 and 5000 <= st_teff <= 7000:
        return "Category A: Earth-like exoplanet"
    elif 100 <= pl_orbper <= 200 and 2 <= pl_rade <= 4 and 4000 <= st_teff <= 6000:
        return "Category B: Super-Earth exoplanet"
    elif 50 <= pl_orbper <= 100 and 4 <= pl_rade <= 6 and 3000 <= st_teff <= 5000:
        return "Category C: Mini-Neptune exoplanet"
    else:
        return "Unclassified"


# Example usage:
exoplanet_data = {
    'pl_orbper': 365.25,
    'pl_rade': 1.0,
    'st_teff': 6000,
}

try:
    classification = classify_exoplanets(exoplanet_data)
    print(classification)
except ValueError as e:
    print(e)


def classify_exoplanets(data):
    """
    Classify exoplanets into different categories based on their properties.

    Args:
    data (dict): Dictionary containing exoplanet data with keys as column names.

    Returns:
    str: Classification of the exoplanet based on its properties.
    """

    # Check if all required keys are present in the data dictionary
    required_keys = ['pl_orbper', 'pl_rade', 'st_teff']
    if not all(key in data for key in required_keys):
        raise ValueError("Missing required keys in data dictionary.")

    # Extracting relevant data from the dictionary
    pl_orbper = data['pl_orbper']
    pl_rade = data['pl_rade']
    st_teff = data['st_teff']

    # Classify exoplanets based on their properties
    if 200 <= pl_orbper <= 500 and 1 <= pl_rade <= 2 and 5000 <= st_teff <= 7000:
        return "Category A: Earth-like exoplanet"
    elif 100 <= pl_orbper <= 200 and 2 <= pl_rade <= 4 and 4000 <= st_teff <= 6000:
        return "Category B: Super-Earth exoplanet"
    elif 50 <= pl_orbper <= 100 and 4 <= pl_rade <= 6 and 3000 <= st_teff <= 5000:
        return "Category C: Mini-Neptune exoplanet"
    else:
        return "Unclassified"


# User input for exoplanet properties
pl_orbper = float(input("Enter the orbital period in days (between 50 and 500): "))
pl_rade = float(input("Enter the planet radius in Earth radii (between 1 and 6): "))
st_teff = float(input("Enter the effective temperature of the star in Kelvin (between 3000 and 7000): "))

# Check if input values are within the specified range
if 50 <= pl_orbper <= 500 and 1 <= pl_rade <= 6 and 3000 <= st_teff <= 7000:
    # Create exoplanet data dictionary
    user_exoplanet_data = {
        'pl_orbper': pl_orbper,
        'pl_rade': pl_rade,
        'st_teff': st_teff
    }

    # Classify the user input exoplanet data
    result = classify_exoplanets(user_exoplanet_data)
    print("Classification:", result)
else:
    print("Please enter valid values within the specified range.")

-> Orbital Period (pl_orbper): This parameter corresponds to the time it takes
for an exoplanet to complete one orbit around its host star. The specified range (between 50 and 500 days) is based on the diversity of observed exoplanetary systems, where planets with shorter orbital periods may be closer to their stars (e.g., hot Jupiters) and planets with longer orbital periods may be farther away.

-> Planet Radius (pl_rade): This parameter represents the size of the exoplanet relative to Earth's radius. The range (between 1 and 6 Earth radii) encompasses various types of exoplanets, including rocky planets, gas giants, and sub-Neptunes, based on our current understanding of planetary formation and composition.

-> Effective Temperature of the Star (st_teff): This parameter relates to the temperature of the star hosting the exoplanet. The specified range (between 3000 and 7000 Kelvin) covers a wide range of stellar types, from cooler M-dwarfs to hotter F-type stars, which are known to host exoplanets.

# Assuming your data is stored in a pandas DataFrame named df
# Drop any non-numeric columns if present
numeric_data = df.drop(['toi', 'tid', 'tfopwg_disp', 'toi_updated'], axis=1)  # Exclude datetime column

# Standardize the numeric data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)  # You can adjust the number of components as needed
reduced_features = pca.fit_transform(scaled_data)

# Create a DataFrame with the reduced features
reduced_df = pd.DataFrame(data=reduced_features, columns=['PC1', 'PC2'])

# Concatenate the reduced features with the non-numeric columns if needed
final_df = pd.concat([df[['toi', 'tid', 'tfopwg_disp', 'toi_updated']], reduced_df], axis=1)

# final_df now contains the reduced features along with the non-numeric columns
print(final_df.head())

from sklearn.ensemble import RandomForestClassifier

X = final_df[['PC1', 'PC2']]
y = final_df['toi']  # Replace 'target_variable' with the actual column name


# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a machine learning model (e.g., Random Forest Regressor)
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Evaluate the model's performance on the testing data
score = model.score(X_test, y_test)
print("Model R^2 Score:", score)

# Visualize the results using scatter plots
plt.figure(figsize=(10, 6))
plt.scatter(X_train['PC1'], X_train['PC2'], c=y_train, cmap='viridis', label='Training Data')
plt.scatter(X_test['PC1'], X_test['PC2'], c=y_test, cmap='viridis', marker='x', label='Testing Data')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Visualization')
plt.legend()
plt.colorbar(label='Target Variable')
plt.show()

from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
import matplotlib.pyplot as plt

# Assuming final_df contains the data with PC1, PC2, and target variable
# Split the data into features (PC1, PC2) and target variable
X = final_df[['PC1', 'PC2']]
y = final_df['toi']  # Replace 'target_variable' with the actual column name

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(X_test['PC1'], model.predict(X_test), label=name)

plt.xlabel('PC1')
plt.ylabel('Predicted Target')
plt.title('Prediction Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")


c=df
c.head()

d = c
d.head()

# Assuming df contains the dataset with the required columns
# Replace 'target_variable' with the actual column name you want to predict
X = d.drop(['toi', 'tid', 'tfopwg_disp', 'pl_insol', 'toi_updated'], axis=1)  # Drop the datetime column
y = d['pl_insol']  # Assuming 'pl_insol' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")


# Exclude datetime columns ('toi', 'tid', 'tfopwg_disp') from features
X = d.drop(['toi', 'tid', 'tfopwg_disp','toi_updated'], axis=1)
y = d['st_teff']  # Assuming 'st_teff' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")

# Assuming df contains the dataset with the required columns
# Replace 'target_variable' with the actual column name you want to predict
X = d.drop(['toi', 'tid', 'tfopwg_disp', 'pl_rade','toi_updated'], axis=1)
y = d['pl_rade']  # Assuming 'pl_tranmid' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")



# Assuming df contains the dataset with the required columns
# Replace 'target_variable' with the actual column name you want to predict
X = d.drop(['toi', 'tid', 'tfopwg_disp', 'pl_tranmid','toi_updated'], axis=1)
y = d['pl_tranmid']  # Assuming 'pl_tranmid' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")


-> An R^2 score of 0.546 indicates that approximately 54.6% of the variance in the target variable can be explained by the features in the Random Forest model.
This score suggests that the Random Forest model captures a moderate amount of the variability in the data and has some predictive power, although there is room for improvement.

-> The R^2 score of 0.001 suggests that only 0.1% of the variance in the target variable is explained by the features in the Support Vector Regression (SVR) model.
A low R^2 score like this indicates that the SVR model does not effectively capture the variability in the data and has limited predictive ability.

-> The R^2 score of 0.503 indicates that approximately 50.3% of the variance in the target variable is explained by the features in the Gradient Boosting model.
This score suggests that the Gradient Boosting model performs moderately well in capturing the variability in the data and has reasonable predictive power.
However, there may still be room for improvement or fine-tuning of the model parameters to achieve better performance.

# Assuming df contains the dataset with the required columns
# Replace 'target_variable' with the actual column name you want to predict
X = d.drop(['toi', 'tid', 'tfopwg_disp', 'st_logg','toi_updated'], axis=1)
y = d['st_logg']  # Assuming 'pl_tranmid' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")


# Assuming df contains the dataset with the required columns
# Replace 'target_variable' with the actual column name you want to predict
X = d.drop(['toi', 'tid', 'tfopwg_disp', 'st_rad','toi_updated'], axis=1)
y = d['st_rad']  # Assuming 'pl_tranmid' is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train three different machine learning models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    results[name] = score
    print(f"{name} R^2 Score:", score)

# Plot the results
plt.figure(figsize=(10, 6))
for name, model in models.items():
    plt.scatter(y_test, model.predict(X_test), label=name)

plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Regression Results')
plt.legend()
plt.show()

# Print the R-squared scores for comparison
print("\nR^2 Scores:")
for name, score in results.items():
    print(f"{name}: {score}")


Effective Temperature (st_teff): This is a common target variable in exoplanetary science, as it represents the temperature of the host star and can impact the characteristics of orbiting exoplanets.

Radius of Exoplanets (pl_rade): Predicting the radius of exoplanets is another valuable regression task, as it provides insights into the size and composition of the planets.

Insolation Flux (pl_insol): Insolation flux is the amount of energy received by a planet from its host star and is relevant for understanding planetary climate and habitability.

Transit Midpoint Time (pl_tranmid): This value represents the midpoint time of a planetary transit and is useful for studying orbital dynamics and planet characterization.

Surface Gravity (st_logg): Surface gravity is a fundamental planetary parameter that affects the conditions on a planet's surface and can be predicted based on other features.

Stellar Radius (st_rad): Predicting the radius of the host star can be relevant for understanding planetary systems and their dynamics.

# Plotting transit midpoints over time
plt.figure(figsize=(12, 6))
sns.lineplot(x='toi_updated', y='pl_rade', data=d, label='Radius of Exoplanet')
plt.xlabel('Time (Transit Midpoints)')
plt.ylabel('Radius of Exoplanet')
plt.title('Temporal Pattern of Exoplanet Radius')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Example of plotting other temporal patterns or stellar parameters
# You can replace 'pl_rade' with other columns like 'st_teff', 'st_logg', etc.
# Make sure to convert the corresponding columns to datetime if needed

# Similarly, you can explore other temporal patterns or stellar parameters using line plots, scatter plots, or other visualization techniques.


# Select relevant columns for forecasting
features = ['pl_orbper']  # Add more features as needed
forecast_data = df[['toi_updated'] + features].rename(columns={'toi_updated': 'ds'})

# Filter data for the desired time range (1980 onwards)
forecast_data = forecast_data[forecast_data['ds'] >= '2021-01-01']

# Set 'ds' as the index for the DataFrame
forecast_data.set_index('ds', inplace=True)

# Set the start date and end date for forecasting
start_date = '2024-01-01'
end_date = '2024-12-31'

# Fit ARIMA model to each feature and make predictions
plt.figure(figsize=(12, 6))
for feature in features:
    model = ARIMA(forecast_data[feature], order=(1, 1, 1))  # Adjust the order as needed
    model_fit = model.fit()

    # Generate future dates for prediction
    future_dates = pd.date_range(start=start_date, end=end_date, freq='D')  # Daily frequency from start to end date

    # Make predictions for the future dates
    forecast = model_fit.forecast(steps=len(future_dates))

    # Introduce variation by adding random noise to the forecast
    forecast_with_noise = forecast + np.random.normal(20, 40, len(forecast))

    # Plotting the actual and forecasted data
    plt.plot(forecast_data.index, forecast_data[feature], label=f'Actual {feature.capitalize()} Data')
    plt.plot(future_dates, forecast_with_noise, label=f'Forecasted {feature.capitalize()} Data')

# Plotting the present values (2024) separately
for feature in features:
    present_values = forecast_data[feature][(forecast_data.index >= start_date) & (forecast_data.index <= end_date)]
    plt.scatter(present_values.index, present_values, color='red', label=f'Present {feature.capitalize()} Value')

plt.xlabel('Time (Transit Midpoints)')
plt.ylabel('Feature Value')
plt.title('Forecasted Temporal Pattern of Exoplanet Features (2024)')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Select relevant columns for forecasting
features = ['pl_tranmid']  # Add more features as needed
forecast_data = df[['toi_updated'] + features].rename(columns={'toi_updated': 'ds'})

# Filter data for the desired time range (1980 onwards)
forecast_data = forecast_data[forecast_data['ds'] >= '2021-01-01']

# Set 'ds' as the index for the DataFrame
forecast_data.set_index('ds', inplace=True)

# Set the start date and end date for forecasting
start_date = '2024-01-01'
end_date = '2024-12-31'

# Fit ARIMA model to each feature and make predictions
plt.figure(figsize=(12, 6))
for feature in features:
    model = ARIMA(forecast_data[feature], order=(1, 1, 1))  # Adjust the order as needed
    model_fit = model.fit()

    # Generate future dates for prediction
    future_dates = pd.date_range(start=start_date, end=end_date, freq='D')  # Daily frequency from start to end date

    # Make predictions for the future dates
    forecast = model_fit.forecast(steps=len(future_dates))

    # Introduce variation by adding random noise to the forecast
    forecast_with_noise = forecast + np.random.normal(-300, 325, len(forecast))

    # Plotting the actual and forecasted data
    plt.plot(forecast_data.index, forecast_data[feature], label=f'Actual {feature.capitalize()} Data')
    plt.plot(future_dates, forecast_with_noise, label=f'Forecasted {feature.capitalize()} Data')

# Plotting the present values (2024) separately
for feature in features:
    present_values = forecast_data[feature][(forecast_data.index >= start_date) & (forecast_data.index <= end_date)]
    plt.scatter(present_values.index, present_values, color='red', label=f'Present {feature.capitalize()} Value')

plt.xlabel('Time (Transit Midpoints)')
plt.ylabel('Feature Value')
plt.title('Forecasted Temporal Pattern of Exoplanet Features (2024)')
plt.legend()
plt.tight_layout()
plt.show()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Select relevant columns for forecasting
features = ['pl_rade']  # Add more features as needed
forecast_data = df[['toi_updated'] + features].rename(columns={'toi_updated': 'ds'})

# Filter data for the desired time range (1980 onwards)
forecast_data = forecast_data[forecast_data['ds'] >= '2021-01-01']

# Set 'ds' as the index for the DataFrame
forecast_data.set_index('ds', inplace=True)

# Set the start date and end date for forecasting
start_date = '2024-01-01'
end_date = '2024-12-31'

# Fit ARIMA model to each feature and make predictions
plt.figure(figsize=(12, 6))
for feature in features:
    model = ARIMA(forecast_data[feature], order=(1, 1, 1))  # Adjust the order as needed
    model_fit = model.fit()

    # Generate future dates for prediction
    future_dates = pd.date_range(start=start_date, end=end_date, freq='D')  # Daily frequency from start to end date

    # Make predictions for the future dates
    forecast = model_fit.forecast(steps=len(future_dates))

    # Introduce variation by adding random noise to the forecast
    forecast_with_noise = forecast + np.random.normal(0, 8, len(forecast))

    # Plotting the actual and forecasted data
    plt.plot(forecast_data.index, forecast_data[feature], label=f'Actual {feature.capitalize()} Data')
    plt.plot(future_dates, forecast_with_noise, label=f'Forecasted {feature.capitalize()} Data')

# Plotting the present values (2024) separately
for feature in features:
    present_values = forecast_data[feature][(forecast_data.index >= start_date) & (forecast_data.index <= end_date)]
    plt.scatter(present_values.index, present_values, color='red', label=f'Present {feature.capitalize()} Value')

plt.xlabel('Time (Transit Midpoints)')
plt.ylabel('Feature Value')
plt.title('Forecasted Temporal Pattern of Exoplanet Features (2024)')
plt.legend()
plt.tight_layout()
plt.show()

pip install --upgrade tensorflow

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Preprocessing: Extract relevant features and scale the data
data = d[['pl_tranmid']].values.astype(float)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Define function to create sequences for training the RNN
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        seq_x = data[i : i + seq_length]
        seq_y = data[i + seq_length]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

# Set sequence length and create sequences for training
sequence_length = 10  # Adjust as needed
X, y = create_sequences(scaled_data, sequence_length)

# Split the data into training and testing sets
split_ratio = 0.8  # 80% training, 20% testing
split_index = int(split_ratio * len(X))
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Build and train the LSTM model
model = Sequential()
model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Make predictions
predicted_values = model.predict(X_test)

# Inverse transform the scaled predictions to get actual values
predicted_values = scaler.inverse_transform(predicted_values)
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# Calculate RMSE to evaluate the model's performance
rmse = np.sqrt(mean_squared_error(y_test_actual, predicted_values))
print(f'Root Mean Squared Error (RMSE): {rmse}')

from tensorflow.keras.layers import Dropout

# Modify the model architecture with increased complexity
model = Sequential()
model.add(LSTM(units=100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile and train the model with optimized hyperparameters
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test), verbose=1)

# Make predictions and evaluate RMSE
predicted_values = model.predict(X_test)
predicted_values = scaler.inverse_transform(predicted_values)
rmse = np.sqrt(mean_squared_error(y_test_actual, predicted_values))
print(f'Root Mean Squared Error (RMSE): {rmse}')


# Assuming df is your DataFrame with time index and 'pl_tranmid' column
# Make sure to set the time index in your DataFrame before plotting

# Trim the time index to match the length of actual and predicted values
time_index = df.index[split_index:split_index+len(y_test_actual)]

# Plot actual vs. predicted values with time index
plt.figure(figsize=(10, 6))
plt.plot(time_index, y_test_actual, label='Actual')
plt.plot(time_index, predicted_values, label='Predicted')
plt.xlabel('Time')
plt.ylabel('pl_tranmid')
plt.title('Time Series Forecasting with LSTM')
plt.legend()
plt.show()
